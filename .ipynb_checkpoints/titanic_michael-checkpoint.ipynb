{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I am working on Kaggle's Titanic dataset. Here I train models to make predictions on the survival of Titanic passengers based upon features such as sex, age and ticket prices. First, I complete some initial data exploration, using some typical Pandas tools, to get a feel and basic understanding of the data. I later do more detailed feature engineering such as null value evaluation, one-hot encoding, and feature scaling. Once the data is processed into X and y training and cross validation data and the models are loaded, predictions are calculated and compared against the true value. The success of these predictions are given by three single valued metrics: accuracy, F1 score and ROC AUC score. These metrics are displayed visually at the bottom of this notebook to display which predictive models worked most successfully. \n",
    "\n",
    "This notebook is inspired by work done by others including other Kaggle users, YouTuber Ken Jee, and help from data scientist friends of mine; a particularly special thanks to my close friend Scott Vinay. I understand all code executed below and feel comfortable replicating similar work in future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Sklearn predictive model modules\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "# Useful finder function\n",
    "def find(s, ch):\n",
    "    return np.array([i for i, ltr in enumerate(s) if ltr == ch]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Data Exploration and Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Dataframe Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we do some initial processing of the dataframe. We make all columns lower case and replace any spaces with underscores for easy and consistent referencing. We remove 'passengerid' as it is only another index.\n",
    "\n",
    "We randomly sample the data to ensure theres no order in the raw data. We also set the random number seed to zero to ensure that when we use the load function we get the same dataframe every time. This is particularly important for when we begin our fold rotations later in the notebook, as it ensures that our training and cross validation data have no overlaps for each distinct fold.\n",
    "\n",
    "Imagine the dataframe is separated into 4 quarters: A, B, C, D. The fold rotation allows each quarter to be the cross validation data once, and its works as follows: in the first case the quarters A, B, C would be the training data and D would be the cross validation data. Following a single fold B, C, D is now the training data and A is the cross validation. This is repeated until each A, B, C, D quarter is the cross validation data once.\n",
    "\n",
    "The modulus operator works well here as it returns the division remainder. When x<y, x%y = x. Once x=y, x%y = 0 and the index list begins growing again from zero. This plays the important role of returning to the A quarter indices once the D quarter indices have been exhausted.\n",
    "\n",
    "To separate the training and cross validation data, we create a new column, 'train'. If a row has a train entry of 1, this is training data. If the entry is 0, this is cross validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "def load(fold=0, mode='train'):   \n",
    "    \"\"\"\n",
    "    Loads dataframe from CSV file and performs initial processing.\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "        fold : int\n",
    "            By default '0'.\n",
    "            This value determines the number of fold rotations executed on the dataframe.\n",
    "            The dataframe is separated into four quarters, so four folds returns the dataframe\n",
    "            back to it's initial configuration.\n",
    "        mode : str\n",
    "            By default 'train'. Options:\n",
    "            'train': Loads training data.\n",
    "            'test' : Loads testing data.\n",
    "    Returns\n",
    "    -------\n",
    "        df : DataFrame\n",
    "            Processed dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Loads dataframe as either training or test data\n",
    "    if mode=='train':\n",
    "        df = pd.read_csv('Data\\train.csv')\n",
    "    elif mode=='test':\n",
    "        df = pd.read_csv('Data\\test.csv')\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # Initial dataframe processing\n",
    "    df      = df.rename(columns = {c:c.lower().replace(' ', '_') for c in df.columns})\n",
    "    df      = df.drop(columns = ['passengerid'])\n",
    "    \n",
    "    # Creating 'train' column to separate training and validation data\n",
    "    # Determinstic random shuffling of the data\n",
    "    np.random.seed(0)\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    indices = df.index\n",
    "    \n",
    "    # Separating dataframe into 75/25 training to cross validation data\n",
    "    tr_idx  = indices[:int(len(indices)*0.75)]\n",
    "    cv_idx  = indices[int(len(indices)*0.75):]\n",
    "    \n",
    "    # Apply a fold rotation, to train and cross validate over whole dataframe when producing results\n",
    "    tr_idx = (tr_idx + int(len(indices)*fold/4))%len(indices)\n",
    "    cv_idx = (cv_idx + int(len(indices)*fold/4))%len(indices)\n",
    "    \n",
    "    # Defining 'train' column\n",
    "    df.loc[tr_idx, 'train']=1\n",
    "    df.loc[cv_idx, 'train']=0\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Look at The Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'Data\\train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-9628c56eecc5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Loading dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-f00b5efbaad1>\u001b[0m in \u001b[0;36mload\u001b[1;34m(fold, mode)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m# Loads dataframe as either training or test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Data\\train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Data\\test.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'Data\\train.csv'"
     ]
    }
   ],
   "source": [
    "# Loading dataframe\n",
    "df = load()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Types and Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolating features containing null values\n",
    "nan_features = []\n",
    "for c in df.columns:\n",
    "    if df[c].isnull().values.any(): \n",
    "        nan_features.append(c)\n",
    "\n",
    "# Percentage of NaNs for each column with null values\n",
    "for nf in nan_features:\n",
    "    nan_percent = 100*df[nf].isnull().sum() /len(df)\n",
    "    print('The feature',nf,f'is {nan_percent:.3} percent NaN values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Columns Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Visualisation of Numerical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating into numeric and categorical data\n",
    "num_cols = ['age', 'sibsp', 'parch', 'fare']\n",
    "\n",
    "# Histogram subplot visualisation of numeric columns\n",
    "fig, axes = plt.subplots(nrows=2,ncols=2,figsize=[10,8])\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Visualisation\n",
    "count=0\n",
    "for ax in axes:\n",
    "    col = num_cols[count]\n",
    "    bins_=[np.arange(0,80,5), np.arange(0,4,0.5), np.arange(0,4,0.5), np.arange(0,175,10), ]\n",
    "    ax.hist(df[col],bins=bins_[count])\n",
    "    ax.set_title(col)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Model might be improved to group sibsp and parch by {0, 1+}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function To Sort Null Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is used to replace the null values. The null evaluation types we have used below are:\n",
    "\n",
    "1 - Zero: This method sorts all null values in the dataframe to zero.\n",
    "\n",
    "2 - Mean: This method calculates the mean of a column using the training data. Then it applies the mean to all null values in the training and cross validation data.\n",
    "\n",
    "3 - Nearest Neighbour: This method creates a symmetric 2x2 matrix of distances, that is the distance, or similarity, between each of the rows. The value at position (i,j) in our matrix indicates the distance between row i and row j in our dataframe.  As the matrix is symmetric, the value at position (i,j) equals the value at position (j,i). First, we identify which column in our dataframe contains null values, e.g. column x. Next, we create the distance matrix using all columns except for column x. Then, we look through column x to find a row, e.g. row y, that contains a null value. We look along row y in the distance matrix to identify the smallest value, which is given at say position (y,z). Finally, you replace the null value in our dataframe at row y, column x, where there was previously a null value, with the value in row z, column x. This process repeats until all null values are replaced with the value in the row that is most similar to itself. \n",
    "\n",
    "There are many different metrics to calculate the distance between rows, here below we have used seuclidean which is the standardised euclidean distance. This differs from the regular euclidean distance by standardising the columns before calculating the distances.\n",
    "\n",
    "Note: naturally the distance matrix has a trace of zero (zeros along the diagonal) so we have set the diagonal values to purposefully large values to prevent the code from selecting diagonal values as the shortest distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sort the null values \n",
    "def fillnull(df, mode = 'zero'):\n",
    "    \"\"\"\n",
    "    Sorts null values of a dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "        df : DataFrame\n",
    "            Dataframe that contains null values to be sorted.\n",
    "            \n",
    "        mode : By default 'zero'. Options:\n",
    "            'zero': Sorts null values into zeros.\n",
    "            'mean': Sorts null values using the mean.\n",
    "            'near': Sorts null values using the nearest neighbour algorithm\n",
    "                    on the rows.\n",
    "    Returns\n",
    "    -------\n",
    "        df : DataFrame\n",
    "            Dataframe with no null values.\n",
    "    \"\"\"\n",
    "    \n",
    "    if mode=='zero':\n",
    "        df = df.fillna(0)\n",
    "\n",
    "    if mode=='mean':\n",
    "        # Mean of the nf column in training dataframe (don't use cv dataframe as this fits data to cv data)\n",
    "        mean      = df.loc[df['train']==1, 'age'].mean()\n",
    "        df['age'] = df['age'].fillna(mean)\n",
    "\n",
    "    # Using nearest neighbours to sort null values\n",
    "    if mode=='near':\n",
    "        df_cols = ['sex', 'sibsp', 'parch', 'fare', 'embarked_C', 'embarked_Q',\n",
    "                   'embarked_S', 'pclass_1', 'pclass_2', 'pclass_3']\n",
    "\n",
    "        # Creating distance matrix to find nearest neighbours\n",
    "        dists   = cdist(df[df_cols].values, df[df_cols].values, 'seuclidean')\n",
    "        dists   = dists + 1e5*np.identity(dists.shape[0])\n",
    "        \n",
    "        # Locates indices of null values\n",
    "        null_idx     = df.loc[pd.isna(df['age']), :].index\n",
    "        non_null_idx = df.loc[~pd.isna(df['age']), :].index\n",
    "\n",
    "        # Loops through rows containing null values\n",
    "        for i in null_idx:\n",
    "            # Distances\n",
    "            distances = list(dists[i, :])\n",
    "            distance_idx_sorted = sorted(\n",
    "                                  range(len(distances)), \n",
    "                                  key=lambda j: distances[j])\n",
    "            for k in range(len(distances)):\n",
    "                if np.isnan(df.loc[k, 'age']) == True:\n",
    "                    continue\n",
    "                else:\n",
    "                    df.loc[i, 'age'] = df.loc[k, 'age']\n",
    "                    break \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Function\n",
    "\n",
    "The process function below formats our features to allow our machine learning models to make predictions on the data.\n",
    "\n",
    "We converted the 'sex' column of male,female into 1,0.\n",
    "\n",
    "To make the fare distribution more uniform we take the logarithm of that feature.\n",
    "\n",
    "We noticed that the age of people with master as their title was lower than the dataframe mean age. We isolated the ages containing master in the title, calculated their mean and replaced the null values in this subset with this mean.\n",
    "\n",
    "We one-hot encoded the columns 'embarked' and 'pclass'. This separates these individual columns into n columns, where n is the number of unique values in each column. E.g. the pclass column had entries {1, 2, 3} depending on the ticket class the individual had. After one-hot encoding there are three separate columns: pclass_1, pclass_2 and pclass_3. In a row where pclass is equal to 1, pclass_1 has an entry of 1 and the other columns have an entry of 0 etc. The same thing is done to the embarked column.\n",
    "\n",
    "Next, we make use of the fillnull function above to take any null values left remaining in the processed dataframe to be replaced. This allows us to use all the rows in the data to make predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing the dataframe\n",
    "def process(df, fillnull_mode='zero'):\n",
    "    \"\"\"\n",
    "    This function processes the dataframe into a format required for predictive models.\n",
    "    It performs feature engineering and separates the dataframe into training and cross validation.\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "        df : DataFrame\n",
    "            Dataframe that requires processing.\n",
    "            \n",
    "        fillnull_mode : By default 'zero'. Options:\n",
    "            'zero': Sorts null values into zeros.\n",
    "            'mean': Sorts null values using the mean.\n",
    "            'near': Sorts null values using the nearest neighbour algorithm\n",
    "                    on the rows.\n",
    "    Returns\n",
    "    -------\n",
    "        dftr : DataFrame\n",
    "            Training dataframe, used to train the predictive model.\n",
    "        dfcv : DataFrame\n",
    "            Cross validation dataframe, used to make and compare predictions\n",
    "            to test success of predictive models.\n",
    "        features : list of strings.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sorting gender column\n",
    "    if 'male' in list(df['sex'].values):\n",
    "        df['sex'] = (df['sex']=='male').astype(int).values\n",
    "        \n",
    "    # Taking the logarithm of the fare column to make the distribution more uniform\n",
    "    df['fare'] = np.log(df.fare+1)\n",
    "    \n",
    "    # Sorting age null values using entries in name col containing str 'Master' \n",
    "    masterAge_mean = df['age'].loc[df['name'].str.contains('Master')].mean()\n",
    "    masterAge_mean = round(masterAge_mean, 2)\n",
    "    df['age'].loc[df['name'].str.contains('Master')] = df['age'].loc[df['name'].str.contains('Master')].fillna(masterAge_mean)\n",
    "    df.loc[df['name'].str.contains('Master'), 'age'] = df.loc[df['name'].str.contains('Master'), 'age'].fillna(masterAge_mean)\n",
    "    \n",
    "    # One-hot encoding 'embarked', 'pclass' columns\n",
    "    # Embarked column\n",
    "    df.dropna(subset=['embarked'])\n",
    "    df_embarked   = pd.get_dummies(df.embarked)\n",
    "    embarked_cols = list(df_embarked.columns)\n",
    "    df_embarked = df_embarked.rename(columns = {c: f'embarked_{c}' for c in embarked_cols})\n",
    "    \n",
    "    # pclass column\n",
    "    df_pclass   = pd.get_dummies(df.pclass)\n",
    "    pclass_cols = list(df_pclass.columns)\n",
    "    df_pclass   = df_pclass.rename(columns = {c: f'pclass_{c}' for c in pclass_cols})\n",
    "\n",
    "    # Removing unnecessary columns\n",
    "    removed_cols = ['name','ticket','embarked','pclass','cabin', 'cabin_number','survived']\n",
    "    df = pd.concat([df, df_embarked, df_pclass], axis=1)\n",
    "    df = df[[c for c in df if c not in removed_cols] + ['survived']]\n",
    "    \n",
    "    # Sorting null values using fillnull\n",
    "    df = fillnull(df, fillnull_mode)\n",
    "    \n",
    "    # Feature scaling: normalisation\n",
    "    for nf in num_cols:\n",
    "        col = df.loc[df['train']==1, nf]\n",
    "        num_col_mean = col.mean()\n",
    "        num_col_std  = col.std()\n",
    "        df[nf] = (df[nf] - num_col_mean)/num_col_std\n",
    "        \n",
    "    # Separating into training and cross validation data\n",
    "    dftr = df[df['train']==1]\n",
    "    dfcv = df[df['train']==0]\n",
    "    \n",
    "    # Removing 'train' column\n",
    "    df = df.drop(columns = ['train'])\n",
    "    \n",
    "    # Finding features\n",
    "    features = df.columns[:-1]\n",
    "\n",
    "    return dftr, dfcv, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataframes and Defining Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_modes(df):\n",
    "    \"\"\"\n",
    "    This function loads data for the different modes.\n",
    "    It makes use of the process function which processes the data and separates it into\n",
    "    training and cross validation data. \n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "        df : DataFrame\n",
    "            Dataframe we are loading data from.\n",
    "    Returns\n",
    "    -------\n",
    "        X_modes : dict\n",
    "            Dictionary containing X data for different modes of null evaluation types.\n",
    "        ytr : array\n",
    "            y training data.\n",
    "        ycv : array\n",
    "            y cross validation data.\n",
    "    \"\"\"\n",
    "    # Training and cross validation data and features\n",
    "    dftr_near, dfcv_near, features = process(df, fillnull_mode = 'near')\n",
    "    dftr_mean, dfcv_mean, features = process(df, fillnull_mode = 'mean')\n",
    "    dftr_zero, dfcv_zero, features = process(df, fillnull_mode = 'zero')\n",
    "\n",
    "    # X training data\n",
    "    Xtr_near = dftr_near[features].values\n",
    "    Xtr_mean = dftr_mean[features].values\n",
    "    Xtr_zero = dftr_zero[features].values\n",
    "    # X cross validation data\n",
    "    Xcv_near = dfcv_near[features].values\n",
    "    Xcv_mean = dfcv_mean[features].values\n",
    "    Xcv_zero = dfcv_zero[features].values\n",
    "\n",
    "    # X data dictionary\n",
    "    X_modes = {\n",
    "        'Near': [Xtr_near, Xcv_near],\n",
    "        'Mean': [Xtr_mean, Xcv_mean],\n",
    "        'Zero': [Xtr_zero, Xcv_zero]}\n",
    "\n",
    "    # y outputs (dftr_mean=dftr_zero and dfcv_mean=dfcv_zero as these values aren't changed by sorting null values)\n",
    "    ytr = dftr_mean['survived'].values\n",
    "    ycv = dfcv_mean['survived'].values\n",
    "\n",
    "    return X_modes, ytr, ycv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting data ready to use below\n",
    "X_modes, ytr, ycv = get_data_modes(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Predictive Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating models\n",
    "model_rf = RandomForestClassifier()\n",
    "model_gb = GradientBoostingClassifier()\n",
    "model_lr = LogisticRegression(max_iter=1e5)\n",
    "\n",
    "# Models dictionary\n",
    "models = {\n",
    "    'Random Forest': model_rf,\n",
    "    'Gradient Boosting': model_gb,\n",
    "    'Logistic Regression': model_lr}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "### Get Result Function\n",
    "\n",
    "The get_results function produces the metric results using the various different models, which indicates how well the models do when predicting the outcome on the cross validation set. The models are first created using the training data, (Xtr, ytr), and then use the cross validation input data, Xcv, to determine the probabilities of the outcomes, y_prob. These probabilities are used to calculate the outcome predictions, y_pred, which here is determining whether a passenger survived. These probabilities, y_prob, and predictions, y_pred, are then compared directly to the true cross validation output data, ycv, and return a single value metric that allows easy comparison between the different models and null evaluation modes. The there metrics we have used below are the accuracy, F1 score and ROC AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(models, X_modes, ytr, ycv):\n",
    "    \"\"\"\n",
    "    Produces the metric results that allow for comparison of models and modes.\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "        models : dict\n",
    "            Dictionary containing predictive models.\n",
    "        X_modes : dict\n",
    "            Dictionary containing input training and cross validation\n",
    "            data for different null evaluation types.\n",
    "        ytr : numpy.ndarray\n",
    "            Array containing output training data.\n",
    "        ycv : numpy.ndarray\n",
    "            Array containing output cross validation data.\n",
    "    Returns\n",
    "    -------\n",
    "        df_results : DataFrame\n",
    "            Dataframe containing the results for the models\n",
    "            and data inputted.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Creating empty dictionary\n",
    "    data = defaultdict(list)\n",
    "    \n",
    "    # Looping through each model\n",
    "    for m in models.keys():\n",
    "        model      = models[m]\n",
    "        # Looping through each set of X data\n",
    "        for x in X_modes.keys():\n",
    "            # Fitting model to X,y training data\n",
    "            X = X_modes[x][0]\n",
    "            y = ytr\n",
    "            model.fit(X,y)\n",
    "            \n",
    "            # y prediction using cross validation data\n",
    "            y_pred = model.predict(X_modes[x][1])\n",
    "            y_prob = model.predict_proba(X_modes[x][1])\n",
    "            \n",
    "            # Evaluation metrics\n",
    "            acc = accuracy_score(y_pred,ycv)\n",
    "            f1  = f1_score(y_pred,ycv)\n",
    "            roc = roc_auc_score(y_prob, ycv)\n",
    "            \n",
    "            # Appending values to dataframe\n",
    "            data['model']   .append(m)\n",
    "            data['mode']    .append(x)\n",
    "            data['accuracy'].append(acc)\n",
    "            data['f1']      .append(f1)\n",
    "            data['roc_auc'] .append(roc)\n",
    "            \n",
    "            # Creating dataframe of results\n",
    "            df_results = pd.DataFrame(data)\n",
    "            \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe of models, null evaluation types and metric scores\n",
    "df_results = get_results(models, X_modes, ytr, ycv)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Result Function\n",
    "\n",
    "This function searches the results dataframe above to extract the best model, null evaluation type and metric score so that it doesn't have to do be done manually. It searches one of the metric columns, finds the maximum value and the index at which the maximum value occurs. It then uses the index to extract the model and mode that correspond to that maximum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_best_results(df_results, metric):\n",
    "    \"\"\"\n",
    "    Finds the best result, along with the model and mode corresponding to it, in the results dataframe. \n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "        df_results : DataFrame\n",
    "            Dataframe of results.\n",
    "        metric : str\n",
    "            By default 'f1'. Options:\n",
    "                'f1': F1 score.\n",
    "                'accuracy': accuracy score.\n",
    "    Returns\n",
    "    -------\n",
    "        best_model : str\n",
    "            Model corresponding to metrics highest score.\n",
    "        best_mode : str\n",
    "            Mode corresponding to metrics highest score.\n",
    "        best_score : str\n",
    "            Best score \n",
    "    \"\"\"\n",
    "    # Finding the best score in the results table\n",
    "    best_score = df_results[metric].max()\n",
    "    idx        = df_results[metric].idxmax()\n",
    "    best_model = df_results.loc[idx, 'model']\n",
    "    best_mode  = df_results.loc[idx, 'mode']\n",
    "    \n",
    "    return best_model, best_mode, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying Best Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(df_results):\n",
    "    \"\"\"\n",
    "    Prints the best result, along with the model and mode corresponding to it, in the results dataframe. \n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "        df_results : DataFrame\n",
    "            Dataframe of results.\n",
    "    Returns\n",
    "    -------\n",
    "        best_model : str\n",
    "            Model corresponding to metrics highest score.\n",
    "        best_mode : str\n",
    "            Mode corresponding to metrics highest score.\n",
    "        best_score : str\n",
    "            Best score \n",
    "    \"\"\"\n",
    "    # Finding best results\n",
    "    best_f1  = finding_best_results(df_results, 'f1')\n",
    "    best_acc = finding_best_results(df_results, 'accuracy')\n",
    "    best_roc = finding_best_results(df_results, 'roc_auc')\n",
    "\n",
    "    # Displaying results\n",
    "    print(f'Best F1 score: {best_f1[2]:.3f} \\nAchieved using {best_f1[0]} and \"{best_f1[1]}\" null evaluation type.\\n')\n",
    "    print(f'Best accuracy: {best_acc[2]:.3f} \\nAchieved using {best_acc[0]} and \"{best_acc[1]}\" null evaluation type.\\n')\n",
    "    print(f'Best Roc Auc: {best_roc[2]:.3f} \\nAchieved using {best_roc[0]} and \"{best_roc[1]}\" null evaluation type.')\n",
    "\n",
    "# Printing results\n",
    "display_results(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising Results\n",
    "\n",
    "The plotter function recieves the results dataframe and plots them in an easily understood bar chat. The models are separated by spaces and the different null value evaluation types are colours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(df, metric='f1'):\n",
    "    \"\"\"\n",
    "    Plots the results of a results dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "        df : DataFrame\n",
    "            Dataframe of results.\n",
    "        metric : str\n",
    "            By default 'f1'. Options:\n",
    "                'f1': F1 score.\n",
    "                'accuracy': accuracy score.\n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # General figure configuration\n",
    "    plt.figure(figsize=(10,5))\n",
    "    sns.set(font_scale=2)\n",
    "    \n",
    "    # Creating lists from dataframe\n",
    "    models         = df['model'].unique()\n",
    "    nan_eval_types = df['mode'].unique()\n",
    "    \n",
    "    # Model separation on graphs\n",
    "    main_ticks = np.arange(len(models))\n",
    "    jump = 1/(len(nan_eval_types)+1)\n",
    "    \n",
    "    # Looping through each null value evaluation type\n",
    "    for i, net in enumerate(nan_eval_types):\n",
    "        # Spacing between models\n",
    "        xpos = main_ticks + jump*i\n",
    "        \n",
    "        # If including error calculation for error bars\n",
    "        if metric+'_std' in df.columns:\n",
    "            plot_data = [df.loc[(df['model']==mod) &\n",
    "                            (df['mode']==net),\n",
    "                            metric+'_mean'].values[0] for mod in models]\n",
    "            # Error data\n",
    "            err_data = [df.loc[(df['model']==mod) &\n",
    "                            (df['mode']==net),\n",
    "                            metric+'_std'].values[0] for mod in models]\n",
    "            plt.bar(\n",
    "                xpos,\n",
    "                plot_data,\n",
    "                yerr = err_data,\n",
    "                width = jump*0.9,\n",
    "                label = net.title(),\n",
    "                capsize=12)\n",
    "\n",
    "        # If error data not included\n",
    "        else:\n",
    "            plot_data = [df.loc[(df['model']==mod) &\n",
    "                                (df['mode']==net),\n",
    "                                metric].values[0] for mod in models]\n",
    "\n",
    "            # Plotting bar charts\n",
    "            plt.bar(xpos,\n",
    "                   plot_data,\n",
    "                   width = jump*0.9,\n",
    "                   label = net.title())\n",
    "    \n",
    "    # Customisation with more specifc customisation depending on metric used\n",
    "    if metric == 'f1':\n",
    "        plt.ylabel(r'$F_1$ score')\n",
    "        plt.title(r'Titanic $F_1$ Scores')\n",
    "    if metric == 'accuracy':\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Titanic Accuracy Scores')\n",
    "    if metric == 'roc_auc':\n",
    "        plt.ylabel('Roc Auc Score')\n",
    "        plt.title('Titanic Roc Auc Score')\n",
    "    # Spacing between different models\n",
    "    plt.gca().set_xticks(main_ticks + jump*(i/2))\n",
    "    \n",
    "    # Legend \n",
    "    legend = plt.legend(title='Null Value Processing Type', loc=[1.01,0.50])\n",
    "    plt.setp(legend.get_title(), fontsize=14)\n",
    "    \n",
    "    # Limits\n",
    "    met_min = df_results[metric].min()\n",
    "    met_max = df_results[metric].max()\n",
    "    buffer  = (met_max - met_min)*0.75\n",
    "    plt.ylim([met_min - buffer, met_max + buffer])\n",
    "    \n",
    "    # General\n",
    "    plt.gca().set_xticklabels(models)\n",
    "    plt.grid(axis='x')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising Final Results Using Bar Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(df_results)\n",
    "plotter(df_results, metric='accuracy')\n",
    "plotter(df_results, metric='roc_auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Including Error bars\n",
    "\n",
    "To account for the possibility of getting 'easy' training and cross validation data that could display better results than are true, we have quartered the dataframe and then varied the rotational fold to collect the quarters together with different permutations, as described at the top of this notebook before the load function. An empty list, results_all, is created and recieves an appended dataframe for each of the four rotational folds that the dataframe experiences. Once completed, these dataframes are concatenated together to created one data frame with all the different models, null evaluation types and metric scores.\n",
    "\n",
    "Next, the dataframe is grouped by model and mode, taking the mean and standard deviation of the accuracy and f1 scores and creating new columns for these values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe of results for each different fold rotation\n",
    "results_all = []\n",
    "for fold in [0,1,2,3]:\n",
    "    # Creating new variation of dataframe\n",
    "    df = load(fold=fold)\n",
    "    \n",
    "    # Loading training and cross validation data \n",
    "    X_modes, ytr, ycv = get_data_modes(df)\n",
    "    \n",
    "    # Calculating results and creating 'fold' column to indicate which rotation the results are associated with\n",
    "    df_results = get_results(models, X_modes, ytr, ycv)\n",
    "    df_results['fold'] = fold\n",
    "    \n",
    "    # Appending results dataframe\n",
    "    results_all.append(df_results)\n",
    "# Combining all data into one dataframe\n",
    "results_all = pd.concat(results_all)\n",
    "\n",
    "# Grouping by model and mode, taking the mean and std of the metrics within each group.\n",
    "results_all = results_all.groupby(['model','mode'])[['accuracy','f1']].agg({'accuracy':['mean','std'], 'f1':['mean','std']}).reset_index(drop=False)\n",
    "results_all.columns = results_all.columns.map('_'.join)\n",
    "\n",
    "# Removing underscores from column names that don't require them\n",
    "results_all = results_all.rename(columns={\n",
    "    'model_':'model',\n",
    "    'mode_':'mode'\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising Results With Error Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results with error bars\n",
    "plotter(results_all, metric='f1')\n",
    "plotter(results_all, metric='accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
